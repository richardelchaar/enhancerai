Here are two recent and effective models with example Python code to win a tabular regression competition predicting `median_house_value` using the `root_mean_squared_error` (RMSE) metric:

```json
[
  {
    "model_name": "LightGBM",
    "example_code": "import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.datasets import make_regression\n\n# Generate synthetic data similar to 'median_house_value' prediction\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42, target_ صدای=10.0)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize LightGBM Regressor with RMSE as the metric\n# LightGBM's default 'regression' objective optimizes for L2 loss (MSE),\n# and 'rmse' metric tracks the root mean squared error during training.\nmodel = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"LightGBM RMSE: {rmse:.4f}\")"
  },
  {
    "model_name": "CatBoost",
    "example_code": "from catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.datasets import make_regression\n\n# Generate synthetic data similar to 'median_house_value' prediction\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42, target_ صدای=10.0)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize CatBoost Regressor with RMSE as the loss function\n# CatBoost natively supports 'RMSE' as a loss function for regression tasks.\nmodel = CatBoostRegressor(loss_function='RMSE', random_seed=42, verbose=0,  # verbose=0 to suppress training output\n                         iterations=100,  # Number of boosting iterations\n                         learning_rate=0.1 # Step size shrinkage\n                         )\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"CatBoost RMSE: {rmse:.4f}\")"
  }
]
```

### Explanation of the Models:

1.  **LightGBM**
    LightGBM (Light Gradient Boosting Machine) is an open-source gradient boosting framework developed by Microsoft that uses tree-based learning algorithms. It is highly efficient and scalable, known for its faster training speed and lower memory consumption compared to other gradient boosting frameworks like XGBoost.

    *   **Effectiveness**: LightGBM builds trees leaf-wise, which can lead to higher accuracy, and utilizes techniques like Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to handle large datasets efficiently. It consistently performs well in tabular data competitions.
    *   **RMSE Metric**: For regression tasks, LightGBM's `objective='regression'` (or `regression_l2`) minimizes the Mean Squared Error (MSE), which is directly related to RMSE. The `metric='rmse'` parameter allows tracking the Root Mean Squared Error during training, making it suitable for the specified competition metric.

2.  **CatBoost**
    CatBoost (Categorical Boosting) is another powerful open-source gradient boosting library developed by Yandex. It stands out for its robust handling of categorical features without requiring extensive preprocessing, its high accuracy, and its resistance to overfitting.

    *   **Effectiveness**: CatBoost employs a novel scheme for processing categorical features, which includes a permutation-driven approach to avoid prediction shift, and "ordered boosting" to prevent overfitting. It also tends to require less hyperparameter tuning for good performance.
    *   **RMSE Metric**: CatBoost directly supports `loss_function='RMSE'` for regression tasks, allowing the model to optimize specifically for this metric during training.

Both LightGBM and CatBoost are state-of-the-art gradient boosting algorithms that are widely used and have demonstrated strong performance in tabular regression tasks in various machine learning competitions. The provided example codes are concise and demonstrate the basic usage of each model for a regression problem, directly calculating RMSE as the evaluation metric.