Here are three recent effective models for tabular regression tasks, commonly used in machine learning competitions, along with concise example codes. Gradient Boosting Machines (GBMs) like LightGBM, XGBoost, and CatBoost consistently rank high in performance for tabular data.

```json
[
  {
    "model_name": "LightGBM Regressor",
    "example_code": """
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# 1. Generate synthetic data
X = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])
y = pd.Series(5 * X['feature_0'] + 2 * X['feature_1'] + np.random.randn(1000) * 0.5)

# 2. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Initialize and train the LightGBM Regressor model
# objective='regression' and metric='rmse' are suitable for this task
lgbm_model = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42)
lgbm_model.fit(X_train, y_train)

# 4. Make predictions
y_pred = lgbm_model.predict(X_test)

# 5. Evaluate the model using RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"LightGBM RMSE: {rmse:.4f}")
"""
  },
  {
    "model_name": "XGBoost Regressor",
    "example_code": """
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# 1. Generate synthetic data
X = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])
y = pd.Series(5 * X['feature_0'] + 2 * X['feature_1'] + np.random.randn(1000) * 0.5)

# 2. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Initialize and train the XGBoost Regressor model
# objective='reg:squarederror' is the default for regression, ev_metric='rmse' for evaluation
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)
xgb_model.fit(X_train, y_train)

# 4. Make predictions
y_pred = xgb_model.predict(X_test)

# 5. Evaluate the model using RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"XGBoost RMSE: {rmse:.4f}")
"""
  },
  {
    "model_name": "CatBoost Regressor",
    "example_code": """
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# 1. Generate synthetic data
X = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])
y = pd.Series(5 * X['feature_0'] + 2 * X['feature_1'] + np.random.randn(1000) * 0.5)

# 2. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Initialize and train the CatBoost Regressor model
# loss_function='RMSE' is directly available for regression
# verbose=0 suppresses training output for conciseness
cat_model = CatBoostRegressor(iterations=100,  # Number of boosting rounds
                              learning_rate=0.1,
                              loss_function='RMSE',
                              random_seed=42,
                              verbose=0) # Set to 0 to suppress output during training
cat_model.fit(X_train, y_train)

# 4. Make predictions
y_pred = cat_model.predict(X_test)

# 5. Evaluate the model using RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"CatBoost RMSE: {rmse:.4f}")
"""
  }
]
```

### 1. LightGBM Regressor

LightGBM (Light Gradient Boosting Machine) is an open-source gradient boosting framework developed by Microsoft that is known for its speed and efficiency, especially with large datasets. It uses tree-based learning algorithms and optimizes for memory usage and training speed, making it a strong choice for competitive tabular regression.

### 2. XGBoost Regressor

XGBoost (Extreme Gradient Boosting) is another highly popular and powerful gradient boosting library that has been a cornerstone in winning many Kaggle competitions. It provides a parallel tree boosting algorithm that is highly efficient, flexible, and portable, offering robust performance in various regression problems.

### 3. CatBoost Regressor

CatBoost (Categorical Boosting) is a gradient boosting library developed by Yandex. A key advantage of CatBoost is its ability to handle categorical features automatically and effectively without requiring extensive preprocessing like one-hot encoding. It often achieves competitive results and provides built-in tools for interpretability.