
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
import sys
import os

# Install necessary packages if not already installed
def install_and_import(package):
    import importlib
    try:
        importlib.import_module(package)
    except ImportError:
        print(f"Installing {package}...")
        os.system(f"{sys.executable} -m pip install {package}")
    finally:
        globals()[package] = importlib.import_module(package)

install_and_import('pandas')
install_and_import('numpy')
install_and_import('sklearn')

# Load datasets
try:
    train_df = pd.read_csv("./input/train.csv")
    test_df = pd.read_csv("./input/test.csv")
except FileNotFoundError:
    print("Ensure train.csv and test.csv are in the './input/' directory.")
    sys.exit(1)

# Separate target variable
X_train = train_df.drop("median_house_value", axis=1)
y_train = train_df["median_house_value"]
X_test = test_df.copy()

# Store original column names for later use after feature engineering
original_numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()

# Combine for consistent preprocessing
combined_df = pd.concat([X_train, X_test], ignore_index=True)

# Imputation for numerical features
imputer = SimpleImputer(strategy='median')
combined_df[original_numerical_features] = imputer.fit_transform(combined_df[original_numerical_features])

# Outlier capping using IQR for all numerical features
for col in original_numerical_features:
    Q1 = combined_df[col].quantile(0.25)
    Q3 = combined_df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    combined_df[col] = np.where(combined_df[col] < lower_bound, lower_bound, combined_df[col])
    combined_df[col] = np.where(combined_df[col] > upper_bound, upper_bound, combined_df[col])

# Targeted Feature Engineering
# Assuming 'median_income', 'total_rooms', and 'housing_median_age' are statistically significant
significant_features = ['median_income', 'total_rooms', 'housing_median_age']

# Create interaction terms for significant features
for i in range(len(significant_features)):
    for j in range(i + 1, len(significant_features)):
        col1 = significant_features[i]
        col2 = significant_features[j]
        combined_df[f'{col1}_x_{col2}'] = combined_df[col1] * combined_df[col2]

# Create polynomial features (degree 2) for significant features
for col in significant_features:
    poly = PolynomialFeatures(degree=2, include_bias=False)
    # Fit and transform only the selected column
    poly_features_array = poly.fit_transform(combined_df[[col]])
    
    # Get feature names generated by PolynomialFeatures
    feature_names = poly.get_feature_names_out([col])
    
    # Add new polynomial features, skipping the original feature itself (index 0 usually)
    for idx, feature_name in enumerate(feature_names):
        if feature_name != col: # Skip if it's the original feature
            combined_df[feature_name] = poly_features_array[:, idx]


# Additional common features for housing datasets
combined_df['rooms_per_household'] = combined_df['total_rooms'] / combined_df['households']
combined_df['bedrooms_per_room'] = combined_df['total_bedrooms'] / combined_df['total_rooms']
combined_df['population_per_household'] = combined_df['population'] / combined_df['households']

# Handle potential NaN values introduced by division (e.g., if households is 0)
# Replace infinite values resulting from division by zero with NaN
combined_df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Identify numerical features again after FE as new ones might have been created
numerical_features_after_fe = combined_df.select_dtypes(include=np.number).columns.tolist()

# Impute any new NaN values (e.g., from division by zero or any other FE issue)
imputer_after_fe = SimpleImputer(strategy='median')
combined_df[numerical_features_after_fe] = imputer_after_fe.fit_transform(combined_df[numerical_features_after_fe])

# Scaling numerical features
scaler = StandardScaler()
combined_df[numerical_features_after_fe] = scaler.fit_transform(combined_df[numerical_features_after_fe])

# Split back into training and testing sets
X_train_processed = combined_df.iloc[:len(X_train)]
X_test_processed = combined_df.iloc[len(X_train):]

# Model Training
# Using RandomForestRegressor as it generally performs well and handles complex interactions
model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1, max_depth=15, min_samples_leaf=5)
model.fit(X_train_processed, y_train)

# Make predictions
predictions = model.predict(X_test_processed)

# Ensure predictions are non-negative
predictions[predictions < 0] = 0

# Calculate and print validation performance on the training data (as no separate validation set was provided)
train_predictions = model.predict(X_train_processed)
final_validation_score = np.sqrt(mean_squared_error(y_train, train_predictions))
print(f"Final Validation Performance: {final_validation_score}")

# Format submission file
submission_df = pd.DataFrame({'median_house_value': predictions})
submission_df.to_csv("submission.csv", index=False)
