## Model name
LightGBM (Light Gradient Boosting Machine)

## Example Python code
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error

# 1. Generate synthetic data (replace with your actual data loading)
# For demonstration, let's create a simple dataset
np.random.seed(42)
X = pd.DataFrame(np.random.rand(100, 5), columns=[f'feature_{i}' for i in range(5)])
y = 2 * X['feature_0'] + 3 * X['feature_1'] - 0.5 * X['feature_2'] + np.random.randn(100) * 0.5
y = pd.Series(y, name='median_house_value') # Target variable

# 2. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Initialize the LightGBM Regressor model
# 'objective=regression' is used for regression tasks, corresponding to L2 loss [6, 7, 11].
# LightGBM builds trees leaf-wise, which can lead to faster convergence but may require careful tuning to avoid overfitting [3, 4, 5, 7, 10, 12, 14, 15, 23].
model_lightgbm = LGBMRegressor(objective='regression', n_estimators=100, learning_rate=0.1, random_state=42)

# 4. Train the model
model_lightgbm.fit(X_train, y_train)

# 5. Make predictions on the test set
y_pred_lightgbm = model_lightgbm.predict(X_test)

# 6. Evaluate the model using Root Mean Squared Error (RMSE)
rmse_lightgbm = np.sqrt(mean_squared_error(y_test, y_pred_lightgbm))
print(f"LightGBM RMSE: {rmse_lightgbm:.4f}")

# LightGBM has many parameters, but using default values and tuning a few like learning_rate,
# num_leaves, and min_data_in_leaf can be effective for performance and to control complexity [10, 15, 16, 19, 23, 25].
