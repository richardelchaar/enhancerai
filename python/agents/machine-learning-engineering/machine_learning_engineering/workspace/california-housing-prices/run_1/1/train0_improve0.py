
# Suppress verbose model output to prevent token explosion
import warnings
warnings.filterwarnings('ignore')
import os
os.environ['PYTHONWARNINGS'] = 'ignore'
# Suppress LightGBM verbosity
os.environ['LIGHTGBM_VERBOSITY'] = '-1'
# Suppress XGBoost verbosity  
os.environ['XGBOOST_VERBOSITY'] = '0'
# Suppress sklearn warnings
from sklearn.utils._testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings('ignore', category=ConvergenceWarning)

import pandas as pd
import numpy as np
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

# Load datasets
train_df = pd.read_csv("./input/train.csv")
test_df = pd.read_csv("./input/test.csv")

# Prepare the data
# Separate target variable from features
X = train_df.drop('median_house_value', axis=1)
y = train_df['median_house_value']

# Identify features for the test set
X_test_submission = test_df.copy()

# Handle missing values (e.g., in 'total_bedrooms') and infinite values
# Identify numerical columns for imputation based on both base and reference solutions
numerical_cols_to_impute = ['total_rooms', 'total_bedrooms', 'population', 'households']

for col in numerical_cols_to_impute:
    if col in X.columns:
        # Replace infinities with NaN first
        X[col] = X[col].replace([np.inf, -np.inf], np.nan)
        # Calculate median only from the training features to prevent data leakage
        median_val = X[col].median()
        X[col].fillna(median_val, inplace=True)
        # Apply the same median to the test set
        if col in X_test_submission.columns:
            X_test_submission[col] = X_test_submission[col].replace([np.inf, -np.inf], np.nan)
            X_test_submission[col].fillna(median_val, inplace=True)

# Feature Engineering: Introduce interaction terms and polynomial features
# using existing numerical columns (e.g., 'total_rooms', 'total_bedrooms', 'population', 'households')
# to enrich the feature set for both training and test data, enhancing the models' ability to capture complex relationships.

# Select numerical columns that are actually present in the data for polynomial feature generation
features_for_poly = [col for col in numerical_cols_to_impute if col in X.columns]

if features_for_poly: # Only proceed if there are columns to transform
    # Initialize PolynomialFeatures to generate degree 2 polynomial features and interaction terms
    # include_bias=False ensures that a constant term (intercept) is not added as a feature.
    poly = PolynomialFeatures(degree=2, include_bias=False)

# Fit PolynomialFeatures on the selected numerical columns of the training data
    # and transform both training and test data.
    # We fit only on training data to prevent data leakage.
    X_poly_transformed = poly.fit_transform(X[features_for_poly])
    
    # Get the names of the new features generated by PolynomialFeatures
    poly_feature_names = poly.get_feature_names_out(features_for_poly)

# Create DataFrames for the new polynomial features
    X_poly_df = pd.DataFrame(X_poly_transformed, columns=poly_feature_names, index=X.index)

# Drop the original numerical columns from X, as PolynomialFeatures(include_bias=False)
    # already includes them (e.g., 'total_rooms' and 'total_rooms^2') in its output.
    X = X.drop(columns=features_for_poly, axis=1)
    # Concatenate the new polynomial features with the rest of X
    X = pd.concat([X, X_poly_df], axis=1)

# Apply the same transformation to the test data using the fitted PolynomialFeatures object
    X_test_poly_transformed = poly.transform(X_test_submission[features_for_poly])
    X_test_poly_df = pd.DataFrame(X_test_poly_transformed, columns=poly_feature_names, index=X_test_submission.index)

# Drop original numerical columns from test set and concatenate new features
    X_test_submission = X_test_submission.drop(columns=features_for_poly, axis=1)
    X_test_submission = pd.concat([X_test_submission, X_test_poly_df], axis=1)

# Fix: One-hot encode 'ocean_proximity' if it exists, as it's a categorical feature
if 'ocean_proximity' in X.columns:
    X = pd.get_dummies(X, columns=['ocean_proximity'], drop_first=True)
if 'ocean_proximity' in X_test_submission.columns:
    X_test_submission = pd.get_dummies(X_test_submission, columns=['ocean_proximity'], drop_first=True)

# Align columns - crucial for consistent feature sets between training and test after one-hot encoding
train_cols = X.columns
test_cols = X_test_submission.columns

missing_in_test = set(train_cols) - set(test_cols)
for c in missing_in_test:
    X_test_submission[c] = 0

missing_in_train = set(test_cols) - set(train_cols)
for c in missing_in_train:
    X[c] = 0

# Ensure the order of columns is the same for both datasets
X_test_submission = X_test_submission[train_cols]
X = X[train_cols]

# Split the training data into training and validation sets
# Using a fixed random_state for reproducibility
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Model Training and Prediction ---

# 1. Initialize and train the LightGBM Regressor model
lgbm = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42, verbose=-1) # verbose=-1 suppresses output
lgbm.fit(X_train, y_train)
y_pred_lgbm = lgbm.predict(X_val)

# 2. Initialize and train the CatBoost Regressor model
catboost = CatBoostRegressor(iterations=100,  # Number of boosting iterations (trees)
                             learning_rate=0.1, # Step size shrinkage
                             depth=6,         # Depth of trees
                             loss_function='RMSE', # Loss function to optimize
                             random_seed=42,    # For reproducibility
                             verbose=False,     # Suppress training output
                             allow_writing_files=False) # Prevent CatBoost from writing diagnostic files
catboost.fit(X_train, y_train)
y_pred_catboost = catboost.predict(X_val)

# --- Ensembling ---
# Simple average ensembling of predictions from LightGBM and CatBoost
y_pred_ensemble = (y_pred_lgbm + y_pred_catboost) / 2

# Evaluate the ensembled model using Root Mean Squared Error (RMSE)
rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_ensemble))

# Print the final validation performance
print(f'Final Validation Performance: {rmse_val}')

# Make predictions on the actual test set for submission
y_pred_lgbm_test = lgbm.predict(X_test_submission)
y_pred_catboost_test = catboost.predict(X_test_submission)

# Ensembled predictions for the test set
final_predictions = (y_pred_lgbm_test + y_pred_catboost_test) / 2

# Create submission file
submission_df = pd.DataFrame(final_predictions, columns=['median_house_value'])
submission_df.to_csv('submission.csv', index=False)