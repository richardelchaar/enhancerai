Here are two recent effective models for tabular regression, along with concise example codes, to excel in competitions evaluated by Root Mean Squared Error (RMSE). Gradient Boosting Decision Tree (GBDT) models, such as LightGBM and CatBoost, consistently demonstrate state-of-the-art performance on tabular data and are frequently employed by winners in machine learning competitions. They often outperform deep learning methods for tabular regression tasks.

```json
[
  {
    "model_name": "LightGBM Regressor",
    "example_code": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n# Generate synthetic data for regression\nX = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])\ny = pd.Series(np.random.rand(1000) * 100 + 5 * X.sum(axis=1))\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize LightGBM Regressor\n# 'objective': 'regression' and 'metric': 'rmse' are suitable for this task.\n# 'n_estimators' can be tuned for better performance.\nmodel_lgbm = lgb.LGBMRegressor(objective='regression', metric='rmse', n_estimators=100, random_state=42)\n\n# Train the model\nmodel_lgbm.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_lgbm = model_lgbm.predict(X_test)\n\n# Calculate RMSE\nrmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\nprint(f\"LightGBM RMSE: {rmse_lgbm:.4f}\")"
  },
  {
    "model_name": "CatBoost Regressor",
    "example_code": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\n\n# Generate synthetic data for regression\nX = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])\ny = pd.Series(np.random.rand(1000) * 100 + 5 * X.sum(axis=1))\n\n# For demonstration, let's add a categorical feature\nX['categorical_feature'] = np.random.choice(['A', 'B', 'C'], size=1000)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical features for CatBoost\ncategorical_features_indices = np.where(X.dtypes == 'object')[0]\n\n# Initialize CatBoost Regressor\n# 'loss_function': 'RMSE' is explicitly set for the competition metric.\n# 'iterations' (n_estimators) can be tuned.\nmodel_catboost = CatBoostRegressor(loss_function='RMSE', iterations=100, random_state=42, verbose=0)\n\n# Train the model, specifying categorical features\nmodel_catboost.fit(X_train, y_train, cat_features=categorical_features_indices)\n\n# Make predictions on the test set\ny_pred_catboost = model_catboost.predict(X_test)\n\n# Calculate RMSE\nrmse_catboost = np.sqrt(mean_squared_error(y_test, y_pred_catboost))\nprint(f\"CatBoost RMSE: {rmse_catboost:.4f}\")"
  }
]
```
### 1. LightGBM Regressor

**Description:** LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework developed by Microsoft that uses tree-based learning algorithms. It is known for its high efficiency, speed, and accuracy, especially on large datasets. It builds trees leaf-wise rather than level-wise, which can lead to faster training and better accuracy. LightGBM is a popular choice in Kaggle competitions for tabular data.

### 2. CatBoost Regressor

**Description:** CatBoost (Categorical Boosting) is another powerful open-source gradient boosting library developed by Yandex. Its key strength lies in its ability to handle categorical features natively, eliminating the need for extensive preprocessing like one-hot encoding. It also uses an ordered boosting scheme to combat prediction shift, leading to more robust models. CatBoost is highly effective for regression tasks and is frequently used in competitions.