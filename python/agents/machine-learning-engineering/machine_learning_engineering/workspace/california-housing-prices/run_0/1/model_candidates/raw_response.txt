For the tabular regression task of predicting `median_house_value` using Root Mean Squared Error (RMSE) as the evaluation metric, LightGBM and XGBoost are two highly effective traditional machine learning algorithms. Both are gradient boosting frameworks known for their performance and efficiency on tabular data.

Here are two effective models with concise example codes:

```json
[
  {
    "model_name": "LightGBM Regressor",
    "example_code": "import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_regression\nimport numpy as np\n\n# Generate synthetic data for demonstration\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize LightGBM Regressor model\n# Objective 'regression' is for L2 loss, 'regression_l1' for L1 loss\n# metric 'rmse' is suitable for the competition\nlgbm_model = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42)\n\n# Train the model\nlgbm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_lgbm = lgbm_model.predict(X_test)\n\n# Evaluate the model using RMSE\nrmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\nprint(f\"LightGBM RMSE: {rmse_lgbm:.4f}\")"
  },
  {
    "model_name": "XGBoost Regressor",
    "example_code": "import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_regression\nimport numpy as np\n\n# Generate synthetic data for demonstration (consistent with LightGBM example)\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize XGBoost Regressor model\n# objective 'reg:squarederror' is standard for regression with L2 loss\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluate the model using RMSE\nrmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\nprint(f\"XGBoost RMSE: {rmse_xgb:.4f}\")"
  }
]
```
These models are popular choices for tabular data due to their ability to handle various data types, capture complex non-linear relationships, and provide good performance with reasonable training times.

### 1. LightGBM Regressor

LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework developed by Microsoft that uses tree-based learning algorithms. It is designed to be faster and more efficient than XGBoost while often achieving comparable or better accuracy. LightGBM builds trees leaf-wise rather than level-wise, which can lead to higher accuracy. It supports parallel and distributed computing and is optimized for speed and memory usage, making it suitable for large datasets.

The example code demonstrates how to set up, train, and evaluate a LightGBM regressor using the scikit-learn API. The `objective='regression'` parameter specifies a regression task with L2 loss, and `metric='rmse'` directly aligns with the competition's evaluation metric.

### 2. XGBoost Regressor

XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It has been a dominant algorithm in many machine learning competitions involving tabular data. XGBoost provides a scikit-learn-compatible API, making it easy to integrate into existing workflows.

The example code initializes an `XGBRegressor` with `objective='reg:squarederror'`, which is the standard objective for regression tasks using squared error. The `eval_metric='rmse'` parameter explicitly sets RMSE as the evaluation metric, aligning with the competition requirements.