Here are two effective traditional machine learning models with concise example codes for predicting `median_house_value` using tabular data, optimized for `root_mean_squared_error` (RMSE). Both models are Gradient Boosting Machines, known for their high performance on tabular datasets.

```json
[
  {
    "model_name": "LightGBM Regressor",
    "example_code": "import lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Sample Data (replace with your actual data loading)\n# For demonstration, creating dummy data similar to housing price prediction\nnp.random.seed(42)\nX = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])\ny = pd.Series(np.random.rand(1000) * 1000000 + 50000, name='median_house_value')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize LightGBM Regressor\n# Common parameters for regression:\n# objective='regression' or 'regression_l1' for MAE, 'regression_l2' for MSE\n# metric='rmse' for Root Mean Squared Error evaluation\n# n_estimators: Number of boosting rounds\n# learning_rate: Step size shrinkage to prevent overfitting\n# num_leaves: Max number of leaves in one tree\nmodel_lgb = lgb.LGBMRegressor(objective='regression_l2', metric='rmse', n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42)\n\n# Train the model\nmodel_lgb.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_lgb = model_lgb.predict(X_test)\n\n# Evaluate the model using RMSE\nrmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(f'LightGBM RMSE: {rmse_lgb:.2f}')"
  },
  {
    "model_name": "XGBoost Regressor",
    "example_code": "import xgboost as xgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Sample Data (replace with your actual data loading)\n# For demonstration, creating dummy data similar to housing price prediction\nnp.random.seed(42)\nX = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])\ny = pd.Series(np.random.rand(1000) * 1000000 + 50000, name='median_house_value')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize XGBoost Regressor\n# Common parameters for regression:\n# objective='reg:squarederror' for regression with squared error (L2 regularization)\n# eval_metric='rmse' for Root Mean Squared Error evaluation\n# n_estimators: Number of boosting rounds\n# learning_rate: Step size shrinkage to prevent overfitting\n# max_depth: Maximum depth of a tree\nmodel_xgb = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)\n\n# Train the model\nmodel_xgb.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_xgb = model_xgb.predict(X_test)\n\n# Evaluate the model using RMSE\nrmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\nprint(f'XGBoost RMSE: {rmse_xgb:.2f}')"
  }
]
```

### Explanation of the Models:

1.  **LightGBM Regressor**
    LightGBM (Light Gradient Boosting Machine) is an open-source gradient boosting framework developed by Microsoft that uses tree-based learning algorithms. It is designed to be highly efficient, fast, and capable of handling large datasets. Its key distinguishing features include Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB), which contribute to its speed and reduced memory usage.
    *   **Effectiveness for Tabular Data:** LightGBM often achieves state-of-the-art performance on tabular datasets due to its efficient implementation of gradient boosting and optimized handling of categorical features. It builds trees leaf-wise, which can lead to higher accuracy compared to level-wise growth but might overfit smaller datasets if not carefully tuned.
    *   **RMSE Optimization:** The `objective='regression_l2'` parameter directly targets the minimization of squared error, which is directly related to RMSE, making it suitable for this competition.

2.  **XGBoost Regressor**
    XGBoost (Extreme Gradient Boosting) is another highly optimized and scalable implementation of gradient boosting trees, renowned for its performance and speed. It was one of the first "big three" gradient boosting frameworks and has been widely used by competition winners on platforms like Kaggle.
    *   **Effectiveness for Tabular Data:** XGBoost is known for its robustness, ability to handle various data types, and effective management of complex feature interactions. It includes regularization terms in its objective function to prevent overfitting, which contributes to its strong generalization capabilities.
    *   **RMSE Optimization:** Similar to LightGBM, `objective='reg:squarederror'` makes XGBoost directly optimize for the squared error, and `eval_metric='rmse'` allows direct monitoring of the Root Mean Squared Error during training, aligning perfectly with the competition's evaluation metric.

Both LightGBM and XGBoost are powerful choices for tabular regression tasks, consistently delivering strong results and are highly recommended for competitions like the one described.