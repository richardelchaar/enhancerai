## Model name
XGBoost Regressor

## Example Python code
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Sample Data (replace with your actual data loading)
# For demonstration, creating dummy data similar to housing price prediction
np.random.seed(42)
X = pd.DataFrame(np.random.rand(1000, 10), columns=[f'feature_{i}' for i in range(10)])
y = pd.Series(np.random.rand(1000) * 1000000 + 50000, name='median_house_value')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize XGBoost Regressor
# Common parameters for regression:
# objective='reg:squarederror' for regression with squared error (L2 regularization)
# eval_metric='rmse' for Root Mean Squared Error evaluation
# n_estimators: Number of boosting rounds
# learning_rate: Step size shrinkage to prevent overfitting
# max_depth: Maximum depth of a tree
model_xgb = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)

# Train the model
model_xgb.fit(X_train, y_train)

# Make predictions on the test set
y_pred_xgb = model_xgb.predict(X_test)

# Evaluate the model using RMSE
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
print(f'XGBoost RMSE: {rmse_xgb:.2f}')