Here are two recent effective models with example codes for tabular regression, focusing on the `root_mean_squared_error` (RMSE) metric. Both LightGBM and XGBoost are gradient boosting frameworks that consistently demonstrate high performance on tabular data tasks and are widely used in machine learning competitions.

```json
[
  {
    "model_name": "LightGBM (Light Gradient Boosting Machine)",
    "example_code": "import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_california_housing\nimport numpy as np\n\n# Load a sample tabular dataset (e.g., California Housing dataset)\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the LGBMRegressor model\n# objective='regression' or 'regression_l1' (MAE), 'regression_l2' (MSE) which is default.\n# We are optimizing for RMSE, which is the square root of MSE, so 'regression_l2' is appropriate.\nmodel_lgbm = lgb.LGBMRegressor(objective='regression_l2', metric='rmse', random_state=42)\n\n# Train the model\nmodel_lgbm.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_lgbm = model_lgbm.predict(X_test)\n\n# Calculate RMSE\nrmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\nprint(f\"LightGBM RMSE: {rmse_lgbm:.4f}\")"
  },
  {
    "model_name": "XGBoost (Extreme Gradient Boosting)",
    "example_code": "import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_california_housing\nimport numpy as np\n\n# Load a sample tabular dataset (e.g., California Housing dataset)\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the XGBRegressor model\n# objective='reg:squarederror' corresponds to mean squared error, which is used for RMSE.\nmodel_xgb = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)\n\n# Train the model\nmodel_xgb.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_xgb = model_xgb.predict(X_test)\n\n# Calculate RMSE\nrmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\nprint(f\"XGBoost RMSE: {rmse_xgb:.4f}\")"
  }
]
```