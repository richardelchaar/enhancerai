## Model name
LightGBM

## Example Python code
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np

# Sample Data Generation (replace with your actual data loading)
np.random.seed(42)
num_samples = 1000
data = pd.DataFrame({
    'feature_1': np.random.rand(num_samples) * 100,
    'feature_2': np.random.rand(num_samples) * 50,
    'feature_3': np.random.randint(0, 5, num_samples),
    'median_house_value': np.random.rand(num_samples) * 500000 + 100000 # Target variable
})

X = data.drop('median_house_value', axis=1)
y = data['median_house_value']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize LightGBM Regressor
# Common parameters for regression task
lgbm = lgb.LGBMRegressor(
    objective='regression_l1', # MAE objective, often robust
    metric='rmse',             # Evaluation metric
    n_estimators=1000,         # Number of boosting rounds
    learning_rate=0.05,        # Step size shrinkage
    num_leaves=31,             # Max number of leaves in one tree
    max_depth=-1,              # No limit on tree depth
    min_child_samples=20,      # Min number of data in a child
    random_state=42,
    n_jobs=-1                  # Use all available cores
)

# Train the model with early stopping
lgbm.fit(X_train, y_train,
          eval_set=[(X_test, y_test)],
          eval_metric='rmse',
          callbacks=[lgb.early_stopping(100, verbose=False)]) # Stop if RMSE doesn't improve for 100 rounds

# Make predictions
y_pred_lgbm = lgbm.predict(X_test)

# Evaluate the model (RMSE)
rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))
print(f'LightGBM RMSE: {rmse_lgbm:.2f}')
